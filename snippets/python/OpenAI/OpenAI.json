{
  "引入 OpenAI Python API 库": {
    "prefix": ["import_openai_", "im_openai_"],
    "body": [
      "from openai import AsyncOpenAI, AsyncStream, OpenAI, Stream  # * OpenAI Python API 库 | Stream 流式返回类",
      "from openai.types.chat.chat_completion import ChatCompletion  # * ChatCompletion 聊天模型类型",
      "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk  # * ChatCompletionChunk 流式返回类型",
      ""
    ],
    "description": ["引入 OpenAI Python API 库"]
  },
  "OpenAI Python 同步|异步 类封装": {
    "prefix": ["import_openai_=all", "im_openai_=all"],
    "body": [
      "import asyncio  # * 异步IO实现更高的并发性",
      "import threading  # * 多线程实现高的并发性",
      "",
      "from openai import AsyncOpenAI, AsyncStream, OpenAI, Stream  # * OpenAI Python API 库 | Stream 流式返回类",
      "from openai.types.chat.chat_completion import ChatCompletion  # * ChatCompletion 聊天模型类型",
      "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk  # * ChatCompletionChunk 流式返回类型",
      "",
      "",
      "class GPT:",
      "    # * class default args(类变量) | signal(信号)",
      "    # 调用方法: [className].[argsName] = [value] | self.[argsName] = [value]",
      "    def __init__(",
      "        self,",
      "        base_url: str = 'http://127.0.0.1:1234',",
      "        api_key: str = 'lm-studio',",
      "        temperature: float = 0.7,",
      "        top_p: float = 0.95,",
      "        frequency_penalty: float = 1.0,",
      "        presence_penalty: float = 1.0,",
      "        stream: bool = True,",
      "    ) -> None:",
      "        \"\"\"",
      "        使用OpenAI API调用AI大语言模型",
      "        :param base_url: API服务器地址",
      "        :param api_key: API密钥",
      "",
      "        :param temperature: 模型温度: 调整单词概率, 使低概率单词更具优势(增加创造力)",
      "        :param top_p: 模型TopP: 直接限制模型的选择范围, 仅保留最可能的单词(增加可预测性)",
      "            低值(如top_p = 0.2)",
      "                只允许模型选择最可能的几个单词",
      "                生成的文本更加保守、可预测, 适用于精确任务(如技术文档、摘要)",
      "            高值(如top_p = 1)",
      "                允许模型从更广泛的词汇选择单词, 增加多样性",
      "                生成的文本更加灵活、创意, 适用于 creative 任务(如创意、游戏、写作)",
      "        !建议: 通常不建议同时调整 Temperature 和 Top-P, 建议只调整其中一个, 以避免不可预测的行为",
      "        !取值范围: [0.0, 1.0]",
      "",
      "        :param frequency_penalty: 频率惩罚: 参数控制 LLM 对已使用词汇的重复率, 避免模型重复使用同一单词, 使其更加灵活(增加创造性)",
      "            正值(如 1.0) -> 降低重复单词的出现率, 避免冗余",
      "            负值(如 -1.0) -> 允许更频繁地重复单词, 适用于诗歌、押韵文本等应用",
      "        !取值范围: [-2.0, 2.0]",
      "",
      "        :param presence_penalty: 存在惩罚: 参数影响 LLM 是否重复使用已经出现的单词, 避免模型生成重复的句子或单词",
      "            正值(如 1.0) -> 促使模型使用新词, 提高文本的多样性",
      "            负值(如 -1.0) -> 允许更多词汇重复, 有助于强调关键词",
      "        !取值范围: [-2.0, 2.0]",
      "",
      "        :param stream: 是否流式返回",
      "        * stream=False, 非流式返回",
      "        * print(response.choices[0].message.content)",
      "        * stream=True的时候, 启用流示返回",
      "        * for chunk in response:",
      "        *     print(chunk.choices[0].delta.content, end='', flush=True)",
      "        \"\"\"",
      "        self.curl = f'{base_url}/v1'  # ! Python的base_url后面自带了 /v1 的参数, 所以base_url后面也要加上 /v1",
      "        self.api_key = api_key",
      "        self.client = OpenAI(api_key=self.api_key, base_url=self.curl)",
      "",
      "        # * 模型参数",
      "        self.temperature = temperature",
      "        self.top_p = top_p",
      "        self.frequency_penalty = frequency_penalty",
      "        self.presence_penalty = presence_penalty",
      "        self.stream = stream",
      "",
      "        # * 用于停止流的 threading.Event",
      "        self._stop_event = threading.Event()",
      "",
      "    def requset_content(self, user_msg: str, model: str):",
      "        \"\"\"",
      "        向指定 AI 模型发送请求",
      "        :param user_msg: 用户输入",
      "        :param model: 模型名称",
      "        \"\"\"",
      "        response: ChatCompletion | Stream[ChatCompletionChunk] = self.client.chat.completions.create(",
      "            messages=[",
      "                {'role': 'user', 'content': user_msg},",
      "            ],",
      "            model=model,",
      "            temperature=self.temperature,",
      "            top_p=self.top_p,",
      "            frequency_penalty=self.frequency_penalty,",
      "            presence_penalty=self.presence_penalty,",
      "            stream=self.stream,",
      "        )",
      "        if isinstance(response, ChatCompletion):",
      "            # 非流式返回处理",
      "            result = response.choices[0].message.content",
      "            print(result)",
      "        else:",
      "            # 流式返回处理",
      "            self.response_stream_content(response)",
      "",
      "    def response_stream_content(self, stream_response: Stream[ChatCompletionChunk]):",
      "        \"\"\"",
      "        流示返回处理",
      "        \"\"\"",
      "        try:",
      "            for chunk in stream_response:",
      "                if self._stop_event.is_set():  # Check if the stop event is set",
      "                    print('\\n流式响应已提前终止')",
      "                    break",
      "                content = chunk.choices[0].delta.content",
      "                if content:",
      "                    self.stream_output_update(content)",
      "                    ...",
      "        finally:",
      "            # 仅支持: raise 语句",
      "            # * 确保关闭连接",
      "            stream_response.response.close()  # 关键: 关闭连接释放资源(仅流式请求需要)",
      "",
      "    def stream_output_update(self, stream_content):",
      "        \"\"\"",
      "        流示数据实时连接处理后刷新输出",
      "        \"\"\"",
      "        print(stream_content, end='', flush=True)",
      "        ...",
      "",
      "    def start(self, user_msg: str, model: str):",
      "        \"\"\"",
      "        向指定 AI 模型发送请求",
      "        :param user_msg: 用户输入",
      "        :param model: 模型名称",
      "        \"\"\"",
      "        self.requset_content(user_msg, model)",
      "        return True",
      "",
      "    def start_thread(self, user_msg: str, model: str):",
      "        \"\"\"",
      "        以线程的方式启动执行",
      "        :param user_msg: 用户输入",
      "        :param model: 模型名称",
      "        \"\"\"",
      "        # * 创建线程来运行请求",
      "        task = threading.Thread(target=self.requset_content, kwargs={'user_msg': user_msg, 'model': model})",
      "        # * 启动线程",
      "        task.start()",
      "        # * 等待请求线程完成",
      "        task.join()",
      "        return True",
      "",
      "    def stop_thread(self):",
      "        \"\"\"",
      "        结束线程",
      "        \"\"\"",
      "        self._stop_event.set()  # ! 设置事件以发出停止信号",
      "        print('\\n终止响应')",
      "        ...",
      "",
      "",
      "class AsyncGPT:",
      "    def __init__(",
      "        self,",
      "        base_url: str = 'http://127.0.0.1:1234',",
      "        api_key: str = 'lm-studio',",
      "        temperature: float = 0.7,",
      "        top_p: float = 0.95,",
      "        frequency_penalty: float = 1.0,",
      "        presence_penalty: float = 1.0,",
      "        stream: bool = True,",
      "    ) -> None:",
      "        \"\"\"",
      "        使用OpenAI API调用AI大语言模型",
      "        :param base_url: API服务器地址",
      "        :param api_key: API密钥",
      "",
      "        :param temperature: 模型温度: 调整单词概率, 使低概率单词更具优势(增加创造力)",
      "        :param top_p: 模型TopP: 直接限制模型的选择范围, 仅保留最可能的单词(增加可预测性)",
      "            低值(如top_p = 0.2)",
      "                只允许模型选择最可能的几个单词",
      "                生成的文本更加保守、可预测, 适用于精确任务(如技术文档、摘要)",
      "            高值(如top_p = 1)",
      "                允许模型从更广泛的词汇选择单词, 增加多样性",
      "                生成的文本更加灵活、创意, 适用于 creative 任务(如创意、游戏、写作)",
      "        !建议: 通常不建议同时调整 Temperature 和 Top-P, 建议只调整其中一个, 以避免不可预测的行为",
      "        !取值范围: [0.0, 1.0]",
      "",
      "        :param frequency_penalty: 频率惩罚: 参数控制 LLM 对已使用词汇的重复率, 避免模型重复使用同一单词, 使其更加灵活(增加创造性)",
      "            正值(如 1.0) -> 降低重复单词的出现率, 避免冗余",
      "            负值(如 -1.0) -> 允许更频繁地重复单词, 适用于诗歌、押韵文本等应用",
      "        !取值范围: [-2.0, 2.0]",
      "",
      "        :param presence_penalty: 存在惩罚: 参数影响 LLM 是否重复使用已经出现的单词, 避免模型生成重复的句子或单词",
      "            正值(如 1.0) -> 促使模型使用新词, 提高文本的多样性",
      "            负值(如 -1.0) -> 允许更多词汇重复, 有助于强调关键词",
      "        !取值范围: [-2.0, 2.0]",
      "",
      "        :param stream: 是否流式返回",
      "        * stream=False, 非流式返回",
      "        * print(response.choices[0].message.content)",
      "        * stream=True的时候, 启用流示返回",
      "        * for chunk in response:",
      "        *     print(chunk.choices[0].delta.content, end='', flush=True)",
      "        \"\"\"",
      "        self.curl = f'{base_url}/v1'",
      "        self.api_key = api_key",
      "        self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.curl)",
      "",
      "        # * 模型参数",
      "        self.temperature = temperature",
      "        self.top_p = top_p",
      "        self.frequency_penalty = frequency_penalty",
      "        self.presence_penalty = presence_penalty",
      "        self.stream = stream",
      "",
      "        # * 异步事件对象, 用于停止流的 asyncio.Event",
      "        self._stop_event = asyncio.Event()",
      "",
      "    async def requset_content(self, user_msg: str, model: str):",
      "        \"\"\"",
      "        向指定 AI 模型发送请求",
      "        :param user_msg: 用户输入",
      "        :param model: 模型名称",
      "        \"\"\"",
      "        self._stop_event.clear()  # * 清除停止事件以便新的请求",
      "        response: ChatCompletion | AsyncStream[ChatCompletionChunk] = await self.client.chat.completions.create(  # * 使用 await",
      "            messages=[",
      "                {'role': 'user', 'content': user_msg},",
      "            ],",
      "            model=model,",
      "            temperature=self.temperature,",
      "            top_p=self.top_p,",
      "            frequency_penalty=self.frequency_penalty,",
      "            presence_penalty=self.presence_penalty,",
      "            stream=self.stream,",
      "        )",
      "        if isinstance(response, ChatCompletion):",
      "            # 非流式返回处理",
      "            result = response.choices[0].message.content",
      "            print(result)",
      "        else:",
      "            # 流式返回处理",
      "            await self.response_stream_content(response)  # * 使用 await 调用异步流处理函数",
      "",
      "    async def response_stream_content(self, stream_response: AsyncStream[ChatCompletionChunk]):",
      "        \"\"\"",
      "        流式返回处理 (异步版本)",
      "        \"\"\"",
      "        try:",
      "            # * 异步迭代器",
      "            async for chunk in stream_response:",
      "                if self._stop_event.is_set():  # * 检查停止事件是否设置",
      "                    print('\\n流式响应终止')",
      "                    break",
      "                content = chunk.choices[0].delta.content",
      "                if content:",
      "                    self.stream_output_update(content)",
      "                    ...",
      "        finally:",
      "            # * 确保关闭连接",
      "            # * AsyncOpenAI 的流式响应在迭代完成后通常会自动关闭, 但显式调用 close() 更安全",
      "            if hasattr(stream_response, 'close') and callable(stream_response.close):",
      "                await stream_response.close()  # 异步连接关闭",
      "",
      "    def stream_output_update(self, stream_content):",
      "        \"\"\"",
      "        流示数据实时连接处理后刷新输出",
      "        \"\"\"",
      "        print(stream_content, end='', flush=True)",
      "        ...",
      "",
      "    async def start_async(self, user_msg: str, model: str):",
      "        \"\"\"",
      "        启动异步",
      "        :param user_msg: 用户输入",
      "        :param model: 模型名称",
      "        \"\"\"",
      "        # * 创建一个任务来运行请求",
      "        task = asyncio.create_task(self.requset_content(user_msg, model))",
      "        await task",
      "        return True",
      "",
      "    def start(self, user_msg: str, model: str):",
      "        \"\"\"",
      "        向指定 AI 模型发送请求",
      "        :param user_msg: 用户输入",
      "        :param model: 模型名称",
      "        \"\"\"",
      "        asyncio.run(self.start_async(user_msg, model))",
      "        return True",
      "",
      "    def stop_async(self):",
      "        \"\"\"",
      "        终止异步",
      "        \"\"\"",
      "        print('\\n-终止异步响应')",
      "        self._stop_event.set()  # * 设置事件以发送停止信号",
      "        return True",
      "",
      "",
      "if __name__ == '__main__':",
      "    \"\"\"",
      "    运行入口 | 调试运行",
      "    \"\"\"",
      "    # ! threadng 多线程",
      "    gpt = GPT('http://127.0.0.1:8080', stream=True)",
      "    gpt.start_thread('你好？', 'deepseek/deepseek-r1-0528-qwen3-8b')",
      "",
      "    # ! asyncio 异步",
      "    async_task = AsyncGPT('http://127.0.0.1:8080', stream=True)",
      "    async_task.start('你好？', 'deepseek/deepseek-r1-0528-qwen3-8b')",
      ""
    ],
    "description": ["描述"]
  }
}
